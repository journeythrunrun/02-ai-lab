# 240405_crawling_02.md
- 김인섭 강사님 내용 기반
  
# 목차 
1. 인사이트  
2. 핵심 코드 정리 타이밍 비교안  
3. crawling 아주 기본만 옮김  
  3.1 코드   
  3.2 기타  
  3.3 체크  

4. crawling 활용 실습, 확장  
  4.1 활용 실습
  4.2 확장
A. selenium
Z. Self Feedback    


-----  
1. 인사이트  
- 송길영 부사장님 만나심
  + 이처럼 인연 및 커넥션

- 특정 키워드 크롤링 -> 주식 텔레그램방에서 사람 당 1000원, 채널 팔기 등
  + 정보를 빠르게 얻는 게 돈이 되기도 함
- 구글 뉴스 크롤링 => AI(자동화) => 자동화(트렌드) # 트랜드 중요

- 면접 시 최신정보 어떻게 얻나요? 크롤링  

- 크롤링_돈 사례 및 취미-> 투자
  + opgg : 취미로 롤 관련된 내용 크롤링 하심    
    -> 시리즈 B투자
  + 유튜브 랭킹 사이트 : 현재 가치 비쌈
  
- 활용되는 곳
  + 논문, 예매, 슬랙봇 

- 휘발되는 데이터들 수집 -> 돈이 됨

- html 보안 취약 # 복사 막아둔 것도 다 가능

- 전자책 -> 5권 -> 달 30만원 
-> 내 분야 베스트셀러 크롤링

- 돈버는 법 : ex) 가습기 추천 : 여러 개 제품 GPT로 요약 -> 쿠팡 연결 -> 수익
- 워드프레스
  -> 하루 3천원 들어오심 / 프롬프트 엔지니어링
- N job은 힘이 약해서 Main job이 중요합

- 워드프로세스 
- 면접에서 회사 데이터분석 : 댓글 수집 하심 (( -> 회사는 뉴스나 비슷한 내용에 따른 빈도수도 가능 ))



## 2. 핵심 코드 정리 타이밍 비교안 ✴️
- A. 당일
  + 누적 학습 ++
- B. 해당 주제 끝난 후
  + 여러 날 학습하며 자연스레 누적 외워지는 시간 save 

- (( 강약 조절 중요, 여러 날 배우면서 중요한 것 위주로 외우기? 특히 면접 질문 가능한 것들 ))

## 3. crawling 아주 기본만 옮김
- 위에 적은 비교안을 아직 선정하지 않았기 때문에 + 날아갔었기 때문에 안 적으려다가 문서의 제목이 crawling이니 조금만 기본만 적는다.

### 3.1 코드  
- tag 속 attribute들 : class 등  
```  
# selenium
from selenium import webdriver # - selenium 의의 ( A. )
from selenium.webdriver.common.by import By
browser=webdriver.Chrome() 
browser.get('https://www.google.com/search?sca_esv=30c375223fbe6df1&q=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5+%EC%B1%97%EB%B4%87&tbm=nws&source=lnms&prmd=invsbmz&sa=X&ved=2ahUKEwiHpOnO9amFAxXEma8BHcfuCRwQ0pQJegQIDhAB&biw=667&bih=650&dpr=1.25'
')
# browser.title # 맞는 곳 갔는지 체크

import pandas as pd
import time

data=[]
for i in range( 1, 10 ) : # 페이지 이동
    # M1) URL : 웹페이지의 페이지들이 구별된 url로 할당 돼있으면 그걸로 아주 많은 페이지도 편하게 가능
    browser.get(f'https://www.yes24.com/Product/Category/BestSeller?categoryNumber=001001003&pageNumber={(page_index + 1)}&pageSize={count_data_in_page}' )  

    # M2) XPATH(페이지 당_규칙성 찾기) & click
    xpath=f'//*[@id="bestContentsWrap"]/div[6]/div/div/div/a[{i}]'
    browser.find_element(By.XPATH, xpath).click()

    # M3) CLASS & click
    browser.find_elements(By.CLASS_NAME,'yesUI_pagen')[-1].find_elements(By.CLASS_NAME,'num')[i].click()
    
    time.sleep(3) # 너무 빨라서 => 웹페이지가 크롤링 막을 수 있음

    parent=browser.find_element(By.CLASS_NAME/ID/XPATH/TAG_NAME, '~')
    container_list=parent.find_elements(By.CLASS_NAME/ID, '~)  # 컨테이너 ~~ 자식들 유사
    # element"s" -> len(container_list) 체크

    for elem in container_list:
        author=elem.find_element(By.CLASS_NAME, 'authPub').text
        link= elem.find_element(By.CLASS_NAME, 'gd_name').get_attribute('href')
    
	data.append({ '저자' : author, '링크' : link } )


df=pd.DataFrame(data)
df
df.to_csv('google_news.csv', encoding='utf-8-sig')  # 윈도우 : 저장된 파일에 외계어 안 뜨려면 인코딩 추가 
# UTF8 vs UTF-8-sig : BOM유무[1] : 파일 읽거나 생성할 때

```

```
# - 기타 함수
browser.find_element(By.ID,'arvRsStnCdNm').clear() # 있던 내용도 비우기
browser.find_element(By.ID,'srchDvNm03').send_keys('01049646003')

# try & refresh()
# 특실 버튼 클릭
#import time
i=1
#t1=time.time()
while(1):
    print(f'{i}번째 시도 중입니다.')
    while(1):
        try :
            left=browser.find_element(By.XPATH,'//*[@id="result-form"]/fieldset/div[6]/table/tbody/tr[1]/td[7]/a').text
        except :
            browser.refresh() #여전히 예매하는 거 없는 빈화면일 땐 리프레쉬말고 아예 이전으로 가야함
        else : 
            left=browser.find_element(By.XPATH,'//*[@id="result-form"]/fieldset/div[6]/table/tbody/tr[1]/td[7]/a').text


    time.sleep(3)

    if left=='매진':
        browser.refresh()
        time.sleep(0.3)
    else:
        print(left)
        browser.find_element(By.XPATH,'//*[@id="result-form"]/fieldset/div[6]/table/tbody/tr[1]/td[7]/a').click()
        break
    i+=1
#t2=time.time()
#print(f'delta t :{(t2-t1)//60}m {(t2-t1)%60}s')
```

```
# 슬랙봇 만들기
# 공지 API : 확장/앱스크립트/API로 봇자동
import requests
import json

# def hello(event, context):
slack_hooks_url ='' # 주피터 노트북 등에 저장

payload = {"text":"Wow WOw"}
headers = {'Content-type': 'application/json'}# content-type 헤더 설정

response = requests.post(slack_hooks_url, data=json.dumps(payload), headers=headers)
print(response.text)#response)
# - 메시지를 특정 시간이나 특정 이벤트가 발생할 때 보내려 하면 서버 필요
# - 파이썬 : 스케줄 모듈

```

### 3.2 기타 
``` 
# - 주의
#   +클래스 이름 사용시 : 개수 혹은 출력되는 것 확인해가며 하기
#     - 검색해보니 3개였음-> m) elements로 찾아서  길이 1인지 체크?

# - 호출 이름 : 띄어쓰기, - 인식 못함 
#   + id 선호 : 고유한 값이기 때문임, 다량의 데이터를 가져오려할 때는 별로임
#   > beautifulsoup도 띄어쓰기 안됨, 이름 겹치기도 함
#   > 겹치는 게 반복문 때 편함


# - 이미지 가져오기
#   링크를 사진으로 다운로드해주는 라이브러리 많이 사용함
# - 구글 html 고도화 돼서 클래스 이름 잘 안 바뀜.
#   > 클래스명 임의의 영어 : 보안을 위해서임

# - browser.할 때 이미 그 html 가져옴

# - 잘 만든 페이지 참고 : 잘 만든 페이지 뭐 사용했는지 보기
#   + 크롬 확장 프로그램 설치 & 창 뜰 때 쿠키 허가 창 없애야 인식됨: https://chromewebstore.google.com/detail/gppongmhjkpfnbhagpmjfkannfbllamg #  https://www.wappalyzer.com/?utm_source=popup&utm_medium=extension&utm_campaign=wappalyzer
#   > Ex. 자라 : 리액트, 구글 태그매니저도 쓰네

# - 통계 마당 : 이런 데이터 팜

# > Xpass 복사 및 활용
#   해당 element에서 마우스 우클릭 > xpass복사
#   > 2페이지 :  //*[@id="bestContentsWrap"]/div[6]/div/div/div/a[1] 
#   > 3페이지 : //*[@id="bestContentsWrap"]/div[6]/div/div/div/a[2] 





# - 상위 클래스 보기
#   + 대상인 하위 클래스 선택 -> 키보드 위 누르며 왼쪽 화면에서 상위클래스 영역 확인 
#   + 키보드 왼쪽 : 상위 클래스로 자동으로 가짐[접어도 짐] / 오른쪽 : 열림

# - .text 사용 시 주의 : 그것을 포함한 상위 태그 및 클래스 이름이 많음. 해당 태그에서 많은 text를 가지고 있을 수 있으므로, 한 방에 확실히 한 개만 가져올 거면 앞뒤로 태그가 닫히는 거 체크


# - text: 큰 따옴표 있든[태그 밖에 있는 text는 "씌워져보임". 근데 복사하면 "없음], 없든 그 문자열을 가져와줌 

# - 브라우저가 로딩 될 때까지 기다리는 방법
#   + (1) 암묵적 대기,
     browser.implicitly_wait(time_to_wait=5) # 5초를 넘기면 오류 발생, 5초 전에 웹페이지를 load하면 다 안 기다리고 넘어가짐
#   + (2) 무작정 대기
     time.sleep(random.randint(1,10))# (5, 30) 많이 쓰심

=> 두 개를 적절하게 잘 섞어서 사용 => 최대한 '인간'처럼

# - 코드에서는 간단히 time.sleep(k) 사용했지만 WebDriverWait를 사용하는 것이 더 바람직합니다[Chat GPT]
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# wait = WebDriverWait(browser, 10) # 최대 10초 대기
# element = wait.until(EC.presence_of_element_located((By.ID, 'element_id')))

# - [에러] StaleElementReferenceException [Chat GPT]
#   ***페이지 바뀌면 browser.에서 새로 값 가져와주기***
#   > Selenium을 사용하여 웹 페이지를 자동화하거나 스크래핑할 때 자주 마주치는 예외 중 하나입니다. 이 예외는 참조하려는 웹 페이지의 요소가 이미 사라졌거나, DOM에서 변경되어 더 이상 유효하지 않을 때 발생합니다. 이는 주로 페이지가 새로 로드되거나 AJAX 호출 등으로 페이지의 일부가 동적으로 변했을 때 자주 발생합니다. 
#   + 페이지에 대한 element(class name 등으로 찾은)는 페이지가 바뀌면 그 elelemnt 내부의 고유한 값은 바뀔 수 있지.
#   + 그 class name으로 다시 찾아주는 거면 모를까, 이전 페이지에서 찾았던 element를 여러 페이지에서 쓰려하면 에러

# - 페이지 줄여서 모바일 버전으로 줄이면 클래스 이름이 바뀜
# - [에러] 화면 잘됐는데 길이 0 : 브라우저 다른 것도 열려 있었음
# - try & except 애용

# - 네이버 뉴스 : 클릭했을 때의 본 뉴스제목   class name 통일성? 

# - naver.com/robots.txt : 자동으로다운

``` 



### 3.3 체크
> 크롤링 (문법 정리), method를 사용하여 표현해 주세요.

### 셀레늄을 사용하여 웹 페이지를 어떻게 로드할 수 있나요?
from selenium import webdriver
from selenium.webdriver.common.by import By

browser=webdriver.Chrome()
browser.get('www.naver.com') 

### 셀레늄으로 웹 페이지에서 특정 요소를 어떻게 찾을 수 있나요? 요소를 찾을 수 있는 함수는 어떤 것들이 있나요? ex) ID, XPATH…
browser.find_element(By.CLASS_NAME, 'abcde') # 2~4) By.ID/XPASS/TAG_NAME
browser.find_elements(By.CLASS_NAME, 'abcde') # len으로 길이 확인

### 셀레늄으로 찾은 요소의 텍스트를 가져오려면 어떻게 해야 하나요?
.text

### 셀레늄에서 특정 텍스트를 입력하고 엔터 키를 누르는 방법은 무엇인가요?
.send_keys('특정 텍스트\n')

### 셀레늄으로 찾은 요소의 속성값을 가져오려면 어떻게 해야 하나요?
- 따옴표 필요  
.get_attribute('href')
### 셀레늄으로 웹 페이지를 스크롤하는 방법은 무엇인가요?
browser.get(url)
### 셀레늄으로 특정 요소를 클릭하는 방법은 무엇인가요?
.click()
### 셀레늄으로 텍스트를 입력하는 방법은 무엇인가요?
.send_keys('텍스트')
### 셀레늄으로 웹 페이지에서 이전 페이지로 이동하는 방법은 무엇인가요?
.find_element(By.XPATH, '~xpass')
### 셀레늄으로 웹 페이지에서 다음 페이지로 이동하는 방법은 무엇인가요?
.find_element(By.XPATH, '~xpass')

### 셀레늄의 Implicit Waits와 Explicit Waits의 차이점은 무엇인가요?
implicitly_wait는 지정한 초 전에 웹페이지를 load하면 다 안 기다리고 넘어가짐
### 구글 플레이 스토어에서 앱 검색 후 댓글을 수집하는 코드를 작성하시오.
- 조건1: 내가 입력한 검색어를 기반으로 첫 번째 앱을 선택합니다.
- 조건2: 댓글 수집이 100개 이상 수집되어야 합니다. (댓글 갯수가 100미만인 앱은 예외)
- 조건3: 댓글에서 아래의 정보를 가져오세요.
    - 작성자명, 별표 갯수, 작성일, 작성 내용
- 조건4: 크롤링한 데이터를 오늘 날짜를 기준으로 엑셀에 저장하시오. (파일명: 2023-04-18)


## 4. crawling 활용 실습, 확장  
### 4.1 활용 실습
```  
# - 카카오 메시지 
#   카카오 메시지 api 검색 -> https://developers.kakao.com/docs/latest/ko/message/rest-api
#   메시지 템플릿

# - 부록
#     [부록1_텔레그램 봇 만들기]

#     [부록2_크롤링 정리]

#     [부록3_텍스트 마이닝이란?]

#     [부록4_100개 이상의 뉴스 제목을 수집 후 단어 빈도수 분석과 시각화 작업]

#     [부록5_크롤링을 위한 준비, 뷰티풀 수프 설치 및 이해]

# - 크롤링 실습
    
#     [[실습]_크롤링 연습예제]

#     [[실습]_구글 뉴스 크롤링]

#     [[실습]_구글 플레이 스토어 댓글을 스크래핑 해보자]

#     [[실습]_RISS 논문 검색 데이터를 스크래핑 해보자]

#     [[실습 ] 넷플릭스 오늘의 Top 10 정보를 크롤링 해봅시다]

#     [[실습] 차
```

### 4.2 확장
- 스케줄러, 시간마다 실행  ( 다른 분이 공유해주심 )
  + https://wikidocs.net/137924  


A. selenium VS beautifulsoup : 누가 많이 사용하는지(마지막 행) 등  [Chat GPT]
![image](https://github.com/journeythrunrun/journeythrunrun/assets/164328543/191f9071-5aa3-4d84-b8af-25dacaf058c9)

## Z. Self Feedback
- 쓰던 거 날아갔다. : 깃허브   
  + 마우스 터치 패드 두 손가락 왼쪽으로 하면 이전으로 가기가 된다.
    >> 스크롤 내리다가 각도 약간 틀어져는지 뒤로가기 되기도 함 + 윈도우 11 현재 버전에서 두 손가락 액션 기능 바꾸는 거 안 보이는 것 같음
  + 이전으로 가기 잘못 입력됐을 시 창이 떠줘서 안 나간다고 누르면 영어 창이 뜬다. 거기서 또 안 나간다고 누르면 나가진다.ㅎ      
  + 쓰지 말라는 계시인가. 원래 썼던 것 만큼 다시 시간 들여 쓰진 않을 것이다. 이 글은 그럴 운명인가봄.
- 실습해보는 거 좋네. 여러가지 중 선택할 때 고민 되는 방향이 다름


References  
[1] BOM  
[https://aubreyjeong.tistory.com/entry/%EC%9C%A0%EB%8B%88%EC%BD%94%EB%93%9C-BOMByte-Order-Mark]
 > 문서 맨 앞에 눈에 보이지 않는 특정 바이트(byte)를 넣은 다음 이것을 해석해서 정확히 어떤 인코딩 방식이 사용되었는지 알아내는 방법을 나타냅니다. 자세하게 유니코드가 little-endian 인지 big-endian 인지 아니면 UTF-8 인지 쉽게 알 수 있도록, 유니코드 파일이 시작되는 첫부분에 보이지 않게, 2~3바이트의 문자열을 추가하는데 이것을 BOM이라고 합니다. BOM은 텍스트 에디터 화면에서는 보이지 않고, 헥사 에디터(Hex Editor)*로 열었을 때만 보입니다.
