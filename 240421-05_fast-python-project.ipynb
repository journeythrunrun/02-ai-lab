{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이썬 프로젝트 빠르게 한 것의 코드 일부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2-2-1\n",
    "# - 추후 하고 싶으면 할 일 (1) 언어 추가 (2) 성능 업그레이드 (3) 추론 속도 증가를 위한 시간 조정 : 빠른 방법이라 사용한 초 제한 말고도 진행\n",
    "\n",
    "# - 참고\n",
    "#   + [1] 허깅페이스 번역 코드 https://huggingface.co/openai/whisper-small\n",
    "#     > [2] 파이프라인 :  https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline\n",
    "#   + [3] 앞처리 코드 : 출처 묻기\n",
    "#   + [4] 그라디오 실시간 STT : https://www.gradio.app/guides/real-time-speech-recognition\n",
    "\n",
    "#   + > [5] 위스퍼 공식 및 언어 정보 (오픈소스) : https://github.com/openai/whisper\n",
    "#   + [6] 영상 음성 : https://mep997.tistory.com/42\n",
    "#   + [7] 더 많은 자료 및 실험은 원본 주피터 노트북, 문서 확인\n",
    "\n",
    "# - [8] 위스퍼 함수 https://huggingface.co/docs/transformers/v4.23.0/en/model_doc/whisper\n",
    "# - 트랜스포머에서 함수 소스 찾음 : [9] https://github.com/huggingface/transformers/blob/v4.23.0/src/transformers/models/whisper/processing_whisper.py#L22\n",
    "\n",
    "import gradio as gr\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" #~\n",
    "print(device)\n",
    "\n",
    "\n",
    "# 추론 속도를 위해 함수 밖으로 빼준 부분 & 코드 실험 시간을 위해 셀 분리\n",
    "# [`WhisperProcessor`] offers all the functionalities of [`WhisperFeatureExtractor`] and [`WhisperTokenizer`]. [5][9]\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "model.to(device)\n",
    "# 트랜스포머에서 함수 소스 찾음 : [9] https://github.com/huggingface/transformers/blob/v4.23.0/src/transformers/models/whisper/processing_whisper.py#L22\n",
    "forced_decoder_ids = torch.tensor(processor.get_decoder_prompt_ids(language=\"english\", task=\"translate\"), dtype=torch.long).to(device)#processor.get_decoder_prompt_ids(language=\"english\", task=\"translate\").to(device)\n",
    "\n",
    "# [1][2]\n",
    "# pipe = pipeline(\n",
    "#   \"automatic-speech-recognition\",\n",
    "#   model=\"openai/whisper-small\",\n",
    "#   device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()\n",
    "def translate_speech(audio_file): # .wav파일\n",
    "    pass # 다른 사람 코드도 조금 들어가 있는데 출처 달기 애매해서 지움\n",
    "\n",
    "# Either 'wav' or 'mp3'. wav files are lossless but will tend to be larger files. mp3 files tend to be smaller. [https://www.gradio.app]\n",
    "interface = gr.Interface(\n",
    "    fn=translate_speech,\n",
    "    inputs=gr.Audio(sources=[\"upload\", \"microphone\"], format=['wav','mp3'], type=\"filepath\", label=\"Record your speech\"),\n",
    "    outputs=gr.Textbox(label=\"Translated Text\"),\n",
    "    title=\"Whisper Translation\",\n",
    "    description=\"Put the Record button. It will translate voices from various languages ​​into English in real time.\")\n",
    "\n",
    "\n",
    "# 최종 사용하진 않은 방법 : 모델 파이프라인으로 가져오기\n",
    "# m1 그라디오 :  transcriber   = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
    "# m2 허깅페이스 - 위스퍼\n",
    "# pipe = pipeline(\n",
    "#   \"automatic-speech-recognition\",\n",
    "#   model=\"openai/whisper-small\",\n",
    "#   chunk_length_s=30,\n",
    "#   device=device,\n",
    "# )\n",
    "\n",
    "def live_translate(stream, new_chunk):\n",
    "    # 그라디오\n",
    "    orig_sr, y = new_chunk # gradio가 주는 sample rate\n",
    "    sr=16000\n",
    "\n",
    "    y = y.astype(np.float32) # 위스퍼 관련 함수 입력 형식을 위해 or 정규화를 위해 int16이면 안됨\n",
    "    y /= np.max(np.abs(y)) #  정규화 -1~1\n",
    "    y = librosa.resample(y, orig_sr=orig_sr, target_sr=sr)\n",
    "\n",
    "    # k초 동안의 샘플 수\n",
    "    num_samples_for_8_sec = int(8 * sr)  # k초 * sample rate\n",
    "    if stream is not None:\n",
    "        stream = np.concatenate( (stream, y))\n",
    "        if len(stream) > num_samples_for_8_sec:\n",
    "            y = stream[-num_samples_for_8_sec:] \n",
    "        else:\n",
    "            y=stream\n",
    "    else:\n",
    "        stream =y\n",
    "    input= np.expand_dims(y, axis=0) #stream에는 추가할 필요없이 보존 데이터일 뿐임\n",
    "    # temp3.append('----1-----')\n",
    "\n",
    "    input_features = processor(input, sampling_rate=sr, return_tensors=\"pt\").input_features.to(device)\n",
    "\n",
    "    # - 모델 입출력\n",
    "    predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "    return stream, transcription[0]\n",
    "    # return stream, pipe(raw=stream, sampling_rate=sr, generate_kwargs = {\"task\":\"translate\", \"language\":\"<|en|>\"} )['text']\n",
    "    # 언어관련 https://github.com/openai/whisper/blob/main/whisper/tokenizer.py\n",
    "    # return stream, pipe({\"sampling_rate\": sr, \"raw\": stream})[\"text\"]\n",
    "live_streaming= gr.Interface(\n",
    "    fn=live_translate,\n",
    "    inputs=[\"state\", gr.Audio(source=\"microphone\", label=\"Record your speech\",streaming=True)],\n",
    "    outputs=[\"state\", gr.Textbox(label=\"Translated Text\"), ],\n",
    "    title=\"Real-time Voice Translation\",\n",
    "    description=\"Put the Record button. It will translate voices from various languages ​​into English in real time.\",\n",
    "    live=True\n",
    ")\n",
    "\n",
    "\n",
    "demo = gr.TabbedInterface([live_streaming, interface], [\"Real-time Voice Translation\", \"Slow Translation\"])\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
